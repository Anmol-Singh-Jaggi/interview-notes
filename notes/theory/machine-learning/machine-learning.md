# Linear regression
- Minimize cost function: Gradient descent/Stochastic Gradient Descent
  - Hypothesis(Prediction) <code>h(theta, X)= (theta)`X</code>
- Normal equation: <code>`Y = [(X`X)^-1]X`Y</code>

# Logistic regression
- Hypothesis <code> h = g[(theta)`x] = 1 / [1 + e^[(theta)`x], g = Sigmoid function</code>

# Neural Networks
# SVM

# Anomaly Detection
- Gaussian distribution
- K-Means Clustering
- KNN
- Different from supervised learning: Large number of -ve examples, few +ve examples.


# Regularization
- Reduce the values of the fitting parameters.
- For solving overfitting (high variance).


# Classification model performance evaluation
- Accuracy not useful if too many samples belong to one class only.
- Therefore use confusion matrix (precision, recall, F1 score, support).
